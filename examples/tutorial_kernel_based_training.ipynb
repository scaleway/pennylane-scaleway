{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This cell is added by sphinx-gallery\n",
    "# It can be customized to whatever you like\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel-based training of quantum models with scikit-learn\n",
    "\n",
    "Over the last few years, quantum machine learning research has provided\n",
    "a lot of insights on how we can understand and train quantum circuits as\n",
    "machine learning models. While many connections to neural networks have\n",
    "been made, it becomes increasingly clear that their mathematical\n",
    "foundation is intimately related to so-called *kernel methods*, the most\n",
    "famous of which is the [support vector machine\n",
    "(SVM)](https://en.wikipedia.org/wiki/Support-vector_machine) (see for\n",
    "example [Schuld and Killoran (2018)](https://arxiv.org/abs/1803.07128),\n",
    "[Havlicek et al. (2018)](https://arxiv.org/abs/1804.11326), [Liu et al.\n",
    "(2020)](https://arxiv.org/abs/2010.02174), [Huang et al.\n",
    "(2020)](https://arxiv.org/pdf/2011.01938), and, for a systematic summary\n",
    "which we will follow here, [Schuld\n",
    "(2021)](https://arxiv.org/abs/2101.11020)).\n",
    "\n",
    "The link between quantum models and kernel methods has important\n",
    "practical implications: we can replace the common [variational\n",
    "approach](https://pennylane.ai/qml/glossary/variational_circuit) to\n",
    "quantum machine learning with a classical kernel method where the\n",
    "kernel---a small building block of the overall algorithm---is computed\n",
    "by a quantum device. In many situations there are guarantees that we get\n",
    "better or at least equally good results.\n",
    "\n",
    "This demonstration explores how kernel-based training compares with\n",
    "[variational\n",
    "training](https://pennylane.ai/qml/demos/tutorial_variational_classifier)\n",
    "in terms of the number of quantum circuits that have to be evaluated.\n",
    "For this we train a quantum machine learning model with a kernel-based\n",
    "approach using a combination of PennyLane and the\n",
    "[scikit-learn](https://scikit-learn.org/) machine learning library. We\n",
    "compare this strategy with a variational quantum circuit trained via\n",
    "stochastic gradient descent using\n",
    "[PyTorch](https://pennylane.readthedocs.io/en/stable/introduction/interfaces/torch.html).\n",
    "\n",
    "We will see that in a typical small-scale example, kernel-based training\n",
    "requires only a fraction of the number of quantum circuit evaluations\n",
    "used by variational circuit training, while each evaluation runs a much\n",
    "shorter circuit. In general, the relative efficiency of kernel-based\n",
    "methods compared to variational circuits depends on the number of\n",
    "parameters used in the variational model.\n",
    "\n",
    "![](https://blog-assets.cloud.pennylane.ai/demos/tutorial_kernel_based_training/main/_assets/static/demonstration_assets/kernel_based_training/scaling.png)\n",
    "\n",
    "If the number of variational parameters remains small, e.g., there is a\n",
    "square-root-like scaling with the number of data samples (green line),\n",
    "variational circuits are almost as efficient as neural networks (blue\n",
    "line), and require much fewer circuit evaluations than the quadratic\n",
    "scaling of kernel methods (red line). However, with current\n",
    "hardware-compatible training strategies, kernel methods scale much\n",
    "better than variational circuits that require a number of parameters of\n",
    "the order of the training set size (orange line).\n",
    "\n",
    "In conclusion, **for quantum machine learning applications with many\n",
    "parameters, kernel-based training can be a great alternative to the\n",
    "variational approach to quantum machine learning**.\n",
    "\n",
    "After working through this demo, you will:\n",
    "\n",
    "-   be able to use a support vector machine with a quantum kernel\n",
    "    computed with PennyLane, and\n",
    "-   be able to compare the scaling of quantum circuit evaluations\n",
    "    required in kernel-based versus variational training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Let us consider a *quantum model* of the form\n",
    "\n",
    "$$f(x) = \\langle \\phi(x) | \\mathcal{M} | \\phi(x)\\rangle,$$\n",
    "\n",
    "where $| \\phi(x)\\rangle$ is prepared by a fixed [embedding\n",
    "circuit](https://pennylane.ai/qml/glossary/quantum_embedding) that\n",
    "encodes data inputs $x,$ and $\\mathcal{M}$ is an arbitrary observable.\n",
    "This model includes variational quantum machine learning models, since\n",
    "the observable can effectively be implemented by a simple measurement\n",
    "that is preceded by a variational circuit:\n",
    "\n",
    "![](https://blog-assets.cloud.pennylane.ai/demos/tutorial_kernel_based_training/main/_assets/static/demonstration_assets/kernel_based_training/quantum_model.png)\n",
    "\n",
    "| \n",
    "\n",
    "For example, applying a circuit $G(\\theta)$ and then measuring the\n",
    "Pauli-Z observable $\\sigma^0_z$ of the first qubit implements the\n",
    "trainable measurement\n",
    "$\\mathcal{M}(\\theta) = G^{\\dagger}(\\theta) \\sigma^0_z G(\\theta).$\n",
    "\n",
    "The main practical consequence of approaching quantum machine learning\n",
    "with a kernel approach is that instead of training $f$ variationally, we\n",
    "can often train an equivalent classical kernel method with a kernel\n",
    "executed on a quantum device. This *quantum kernel* is given by the\n",
    "mutual overlap of two data-encoding quantum states,\n",
    "\n",
    "$$\\kappa(x, x') = | \\langle \\phi(x') | \\phi(x)\\rangle|^2.$$\n",
    "\n",
    "Kernel-based training therefore bypasses the processing and measurement\n",
    "parts of common variational circuits, and only depends on the data\n",
    "encoding.\n",
    "\n",
    "If the loss function $L$ is the [hinge\n",
    "loss](https://en.wikipedia.org/wiki/Hinge_loss), the kernel method\n",
    "corresponds to a standard [support vector\n",
    "machine](https://en.wikipedia.org/wiki/Support-vector_machine) (SVM) in\n",
    "the sense of a maximum-margin classifier. Other convex loss functions\n",
    "lead to more general variations of support vector machines.\n",
    "\n",
    "> > Note\n",
    ">\n",
    "> More precisely, we can replace variational with kernel-based training\n",
    "> if the optimisation problem can be written as minimizing a cost of the\n",
    "> form\n",
    ">\n",
    "> $$\\min_f  \\lambda\\;  \\mathrm{tr}\\{\\mathcal{M}^2\\} + \\frac{1}{M}\\sum_{m=1}^M L(f(x^m), y^m),$$\n",
    ">\n",
    "> which is a regularized empirical risk with training data samples\n",
    "> $(x^m, y^m)_{m=1\\dots M},$ regularization strength\n",
    "> $\\lambda \\in \\mathbb{R},$ and loss function $L.$\n",
    ">\n",
    "> Theory predicts that kernel-based training will always find better or\n",
    "> equally good minima of this risk. However, to show this here we would\n",
    "> have to either regularize the variational training by the trace of the\n",
    "> squared observable, or switch off regularization in the classical SVM,\n",
    "> which removes a lot of its strength. The kernel-based and the\n",
    "> variational training in this demonstration therefore optimize slightly\n",
    "> different cost functions, and it is out of our scope to establish\n",
    "> whether one training method finds a better minimum than the other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel-based training\n",
    "\n",
    "First, we will turn to kernel-based training of quantum models. As\n",
    "stated above, an example implementation is a standard support vector\n",
    "machine with a kernel computed by a quantum circuit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by importing all sorts of useful methods:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pennylane as qml\n",
    "from pennylane.templates import AngleEmbedding\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step is to define a data set. Since the performance of the\n",
    "models is not the focus of this demo, we can just use the first two\n",
    "classes of the famous [Iris data\n",
    "set](https://en.wikipedia.org/wiki/Iris_flower_data_set). Dating back to\n",
    "as far as 1936, this toy data set consists of 100 samples of four\n",
    "features each, and gives rise to a very simple classification problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# pick inputs and labels from the first two classes only,\n",
    "# corresponding to the first 100 samples\n",
    "X = X[:100]\n",
    "y = y[:100]\n",
    "\n",
    "# scaling the inputs is important since the embedding we use is periodic\n",
    "scaler = StandardScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# scaling the labels to -1, 1 is important for the SVM and the\n",
    "# definition of a hinge loss\n",
    "y_scaled = 2 * (y - 0.5)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the [angle-embedding\n",
    "template](https://pennylane.readthedocs.io/en/stable/code/api/pennylane.templates.embeddings.AngleEmbedding.html)\n",
    "which needs as many qubits as there are features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_qubits = len(X_train[0])\n",
    "n_qubits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the kernel we could prepare the two states\n",
    "$| \\phi(x) \\rangle,$ $| \\phi(x') \\rangle$ on different sets of qubits\n",
    "with angle-embedding routines $S(x), S(x'),$ and measure their overlap\n",
    "with a small routine called a [SWAP\n",
    "test](https://en.wikipedia.org/wiki/Swap_test).\n",
    "\n",
    "However, we need only half the number of qubits if we prepare\n",
    "$| \\phi(x)\\rangle$ and then apply the inverse embedding with $x'$ on the\n",
    "same qubits. We then measure the projector onto the initial state\n",
    "$|0..0\\rangle \\langle 0..0|.$\n",
    "\n",
    "![](https://blog-assets.cloud.pennylane.ai/demos/tutorial_kernel_based_training/main/_assets/static/demonstration_assets/kernel_based_training/kernel_circuit.png)\n",
    "\n",
    "To verify that this gives us the kernel:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\begin{align*}\n",
    "    \\langle 0..0 |S(x') S(x)^{\\dagger} \\mathcal{M} S(x')^{\\dagger} S(x)  | 0..0\\rangle &= \\langle 0..0 |S(x') S(x)^{\\dagger} |0..0\\rangle \\langle 0..0| S(x')^{\\dagger} S(x)  | 0..0\\rangle  \\\\\n",
    "    &= |\\langle 0..0| S(x')^{\\dagger} S(x)  | 0..0\\rangle |^2\\\\\n",
    "    &= | \\langle \\phi(x') | \\phi(x)\\rangle|^2 \\\\\n",
    "    &= \\kappa(x, x').\n",
    "\\end{align*}\n",
    "\\end{aligned}$$\n",
    "\n",
    "Note that a projector $|0..0 \\rangle \\langle 0..0|$ can be constructed\n",
    "using the `qml.Hermitian` observable in PennyLane.\n",
    "\n",
    "Altogether, we use the following quantum node as a *quantum kernel\n",
    "evaluator*. For this purpose, we will use the `scaleway.aer` device from\n",
    "the Scaleway plugin for PennyLane. Define the following credentials directly in the cell or set them as environment variables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "project_id = os.getenv(\"SCW_PROJECT_ID\")\n",
    "secret_key = os.getenv(\"SCW_SECRET_KEY\")\n",
    "backend_name = \"EMU-AER-16C-128M\"\n",
    "\n",
    "print(f\"Project ID: {project_id}\")\n",
    "print(f\"Secret Key: {secret_key}\")\n",
    "print(f\"Backend Name: {backend_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define our pennylane's quantum node along with the chosen device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dev_kernel = qml.device(\n",
    "    \"scaleway.aer\",\n",
    "    wires=n_qubits,\n",
    "    project_id=project_id,\n",
    "    secret_key=secret_key,\n",
    "    backend=backend_name,\n",
    ")\n",
    "\n",
    "\n",
    "@qml.qnode(dev_kernel)\n",
    "def kernel(x1, x2):\n",
    "    \"\"\"The quantum kernel.\"\"\"\n",
    "    AngleEmbedding(x1, wires=range(n_qubits))\n",
    "    qml.adjoint(AngleEmbedding)(x2, wires=range(n_qubits))\n",
    "    return qml.probs(wires=range(n_qubits))\n",
    "\n",
    "\n",
    "def kernel_wrapper(x1, x2):\n",
    "    \"\"\"x1, x2 can be floats or arrays.\"\"\"\n",
    "    return kernel(x1, x2)[..., 0]\n",
    "\n",
    "\n",
    "qml.draw_mpl(kernel, decimals=1, style=\"pennylane\")(X_train[0], X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good sanity check is whether evaluating the kernel of a data point and\n",
    "itself returns 1:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kernel_wrapper(X_train[0], X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way an SVM with a custom kernel is implemented in scikit-learn\n",
    "requires us to pass a function that computes a matrix of kernel\n",
    "evaluations for samples in two different datasets A, B. If A=B, this is\n",
    "the [Gram matrix](https://en.wikipedia.org/wiki/Gramian_matrix).\n",
    "\n",
    "We define our kernel matrices function in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kernel_matrix(A, B):\n",
    "    \"\"\"Compute the matrix whose entries are the kernel\n",
    "    evaluated on pairwise data from sets A and B.\"\"\"\n",
    "    inputs = np.array([[a, b] for a in A for b in B])\n",
    "    return kernel_wrapper(inputs[:, 0], inputs[:, 1]).reshape(len(A), len(B))\n",
    "\n",
    "\n",
    "def kernel_matrix_training_optimized(A):\n",
    "    \"\"\"\n",
    "    Optimized version of kernel_matrix for training data.\n",
    "    We can take advantage of the fact that in this case, we compute the Gram matrix.\n",
    "    \"\"\"\n",
    "    # This is a bit of a hack to make sure we don't compute the same kernel evaluation twice, the Gram matrix being symmetric.\n",
    "    # We avoid unecessary calls to make the training faster and less costly.\n",
    "\n",
    "    inputs = []\n",
    "    for i, a in enumerate(A):\n",
    "        for j, b in enumerate(A[i:], i):\n",
    "            if i == j:\n",
    "                continue\n",
    "            inputs += [[a, b]]\n",
    "\n",
    "    inputs = np.array(inputs)\n",
    "    K_upper = kernel_wrapper(inputs[:, 0], inputs[:, 1])\n",
    "\n",
    "    K = np.ones((len(A), len(A)))\n",
    "    K[np.triu_indices_from(K, k=1)] = K_upper\n",
    "    K.T[np.triu_indices_from(K.T, k=1)] = K_upper\n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick sanity check on 5 data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = kernel_matrix_training_optimized(X_train[:5])\n",
    "plt.imshow(mat)\n",
    "plt.colorbar()\n",
    "plt.title(\"Kernel matrix on sample data (5 points)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the full kernel matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gram_matrix = kernel_matrix_training_optimized(X_train)\n",
    "print(gram_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation should take ~22 minutes (~2.1 circuits evaluated per second - 0.476 s/circ). Let's take a look just for fun:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(gram_matrix)\n",
    "plt.colorbar()\n",
    "plt.title(\"Kernel matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the SVM optimizes internal parameters that basically weigh\n",
    "kernel functions. It is a breeze in scikit-learn, which is designed as a\n",
    "high-level machine learning library.\n",
    "Let's compute the accuracy on the test set once training is done.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with dev_kernel.tracker:\n",
    "    svm = SVC(kernel=lambda x, y: gram_matrix).fit(X_train, y_train)\n",
    "    svm.set_params(kernel=kernel_matrix)\n",
    "    predictions = svm.predict(X_test)\n",
    "    accuracy = accuracy_score(predictions, y_test)\n",
    "\n",
    "print(f\"predictions.shape = {predictions.shape}\")\n",
    "print(f\"y_test.shape = {y_test.shape}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We stop the device in order to stop our remote session and avoid unecessary cost!\n",
    "dev_kernel.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM predicted all test points correctly. How many times was the\n",
    "quantum device evaluated during training and testing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dev_kernel.tracker.totals[\"executions\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
